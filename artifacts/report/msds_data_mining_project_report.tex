\documentclass[sigconf, authorversion, nonacm]{acmart}

\settopmatter{printacmref=false} % remove acm info that clutters things up for this project
\settopmatter{printfolios=true} % add page numbers

%\citestyle{acmauthoryear}

\usepackage{hyperref}

\begin{document}

\title{Higher Education Synthesized}
\subtitle{Extending Higher Education Data with Synthetic Data at the Onset of the Demographic Cliff}

\begin{abstract}
    American higher education is coming to an impasse. Colleges and universities have seen enrollments grow as larger generations of Americans made their way through postsecondary education. Schools increased their academic offerings and student life activities. Faculty and staff members filled the needs for supporting increasing student enrollments. But there was an issue hiding behind all of this growth: what happens when student enrollments systematically fall?

    The United States has seen declines in high school graduates and, therefore, reduced college applicants. Depending on who you listen to, this ranges from a demographic cliff ushering in an emergency in higher education to a slower decline in higher education enrollments that will play out over years. It is unclear what effect this will have on the near future of higher education. One expected consequence of these declines is that we will see smaller amounts of higher education data relative to previous years.

    This project focuses on synthesizing Integrated Postsecondary Education Data System data from the National Center for Education Statistics. This data covers a vast range of information about higher education, and, while we will not look at student-level data --- one of the key areas for synthetic data after the demographic cliff hits --- we will start by generating synthetic data based on subsets of institution-related data. The goal is to show how we might address the diminishing data after the demographic cliff in order to continue higher education data analysis that relies on larger datasets.
\end{abstract}

\maketitle

\section{Introduction}
    American higher education saw growing enrollments throughout the latter half of the 1900s and moving into the 2000s. This resulted in increases in the number of higher education institutions (HEIs) in the country, including public and private colleges and universities. These HEIs span from major research institutions down to small liberal arts schools. This has also resulted in increases in the number of faculty and staff to support growing enrollments. The general trend in higher education was growth.

    The issue is that this growth in enrollments was not to last. For years now, we have seen predicted and then realized declines in high school graduates along with the respective drop in higher education enrollments. Higher education's future was based on growth, but then the key aspect of that growth --- more students than in previous years --- fell away. Estimates for what comes next vary. Some see a demographic cliff and a looming emergency in which declining enrollments result in catastrophic cuts at and closures of HEIs. It is true that we have seen increasing numbers of HEI closures in recent years ~\cite{hechingercollegeclosures}. But others see the trend as a more gradual decline, giving higher education time to adapt and shrink accordingly. \cite{insidehighereddemographiccliff}

    There are many interesting research questions related to the future of higher education, appropriate and responsible responses to enrollment declines, and other aspects of this impending reality. There are also questions related to the shifting of costs from public funding to individual students, the shift in emphasis to higher education as workforce preparation, the modern publishing culture in higher education and how that affects the student experience, and more.

    This project will instead focus on a different aspect of declining higher education enrollments: reduced data. As student enrollments drop and HEIs close or shrink, we are likely to see reduced amounts of higher education data available for analysis, prediction, evaluation, and more. Not all analyses in higher education need larger amounts of data, but we want to explore how we could supplement real data with synthetic data in cases where we need more overall data. The risk is that, without sufficient data, we may not be able to support students and HEIs as well as we could with larger amounts of data.

    We will take a look at Integrated Postsecondary Education Data System (IPEDS) data from the National Center for Education Statistics (NCES), housed in the U.S. Department of Education's Institute of Education Sciences (IES). NCES gathers, prepares, and makes available data along with providing analysis on education data. IPEDS is a subset of NCES data that focuses on postsecondary or higher education data. IPEDS data is large, complex, and valuable, so it also lends itself well to data mining. \cite{ipeds}

    We will focus on a subset of IPEDS data related to institutions, enrollments, and graduation rates. For each of these three domains, IPEDS offers multiple files covering different information, and it provides these for multiple years. Our goal will be to identify one or more analysis or modeling datasets that we can pull out of a larger extract-transform-load (ETL) pipeline with multiple repeated steps. We will focus on pulling IPEDS data, loading it into a relational database in its raw form, preprocessing it in the database with a series of automated processes, storing the analysis or modeling datasets in the database, and then using those datasets to synthesize new data.

   To address the issue of the amount of data available to us, we will leverage generative modeling to produce synthetic tabular data. The goal is to build a system that can demonstrate how we might leverage synthetic higher education data to fill in datasets in the future as they potentially shrink due to the issues described above.

    We are focusing on institution-level data from IPEDS, but future iterations of this work could pivot to generating synthetic student-level data.

    Synthetic data is ``artificially generated [data] that resemble[s] the actual data --- more precisely, having similar statistical properties'' ~\cite[p. 1]{decristofaro2024syntheticdatamethodsuse}. If we generate synthetic IPEDS data that shares the same properties as the real data, then we can combine the real and synthetic data, or, alternatively, we can create an indefinite amount of synthetic data, to increase the amount of data we have to work with.

    Building and training models to generate synthetic tabular data --- tabular meaning data in table form with rows and columns --- is notoriously difficult. We will leverage existing models and research for the synthetic generation aspect of the pipeline in particular. We will lean on the research in the Modeling Tabular Data Using Conditional GAN paper ~\cite{DBLP:journals/corr/abs-1907-00503} and the associated Python libraries they make available at the Synthetic Data Vault Project ~\cite{sdv}.

    While we will not dive too deep into the working of the generative models, it can be useful to understand the complexity in these and why we are leveraging existing implementations. The research paper focuses on their conditional tabular generative adversarial network (CTGAN) approach to generative modeling. This paper provides some examples of the difficulty of modeling for tabular data. Section 3 speaks to some of these issues:

    \begin{itemize}
        \item Mixed data types: tabular data can have features with continuous, nominal, ordinal, binary, or other data. Generative modeling for these datasets require handling mixtures of data types in one model.
        \item Non-Gaussian distributions: continuous data can be non-Gaussian, so attempts to standardize or min-max continuous features can result in issues such as vanishing or exploding gradients.
        \item Multimodal distributions: continuous features may have more than one mode, resulting in the need to model more than one mode with approaches such as kernel density estimation and Gaussian mixture models.
        \item Imbalance in categories for categorical features: categories that have smaller representation may not be modeled as accurately as those with larger representation, resulting in the need for extra steps to handle this disparity. \cite{DBLP:journals/corr/abs-1907-00503}
    \end{itemize}

    We do not need to go into the inner workings of these issues. Instead, they are meant to provide a snippet of the difficulties we would need to handle if building our own generative models. In order to remain focused on the pipeline in this project, we will, as said above, leverage existing generative modeling libraries available through the Synthetic Data Vault Project ~\cite{sdv}.

    Finally, the ultimate goal is to glean insights from the IPEDS data, real or synthesized. The synthetic data will ideally give us the same overall insights as the real data. For this task, we can turn to traditional data mining techniques: classification, clustering, association mining, rule mining, or others. If we can determine trends now, we may be able to utilize synthetic data in the future with dwindling real data to confirm continued trends or to identify changes in those trends. Since the focus here is on synthetic data, we will spend more of our time testing out data mining techniques on the real and synthetic data and confirming that we see similar results, supporting use of synthetic data as a stand-in for or supplement to real data.

    To summarize, this project will work with IPEDS data with its focus on higher education and will explore how we can synthesize more IPEDS data to address potential future lack-of-data issues due to declining higher education enrollments, and we will use these data to tackle standard data mining questions focused on higher education data but across real and synthetic data.

\section{Related Work}
    My literature review did not return as much related to data mining in higher education as I expected to see, and I was unable to find any research closely related to what we explore in this project. The bulk of what I found is related to different evaluation methods for student learning. Examples are looking at adaptive learning --- learning that dynamically adjusts to students based on different input metrics --- and performance prediction --- trying to determine how a student will end up performing by, for example, graduation. While these are interesting, they do not focus on the type of data that we see in IPEDS.

    Note that some of these papers were already referenced in the introduction above.

    To start with, ~\cite{sdp} by the Harvard Strategic Data Project (SDP) fellowship program speaking about the importance of analytics in higher education data. There are larger questions that this paper raises about why higher education appears behind other areas in terms of leveraging data and more advanced analytics.

    Another example of an interesting paper that I want to highlight is ~\cite{Hassna02102023} that explores utilizing big data and analytics for higher education data. The catch is that this paper is from 2022, and it is talking about introducing big data and analytics in higher education, mirroring some of what we see from the SDP paper above in hinting that higher education may be behind other areas in terms of utilizing data. But, during the literature review, I checked out big data and data mining and related terms in other domains, and I did not tend to see papers talking about how big data and analytics have yet to be realized in those domains.

    Next is a handful of papers characterizing what I mostly found: research into specific uses of analytics and mining to hone in on an aspect of student evaluation. Since our focus on this project is on institution-level information with IPEDS data, we are zooming out quite a bit from what these papers cover, but these are useful in terms of getting bearings for research on higher education data mining, though none of these papers are overly useful for the specific use case in this project.

    First is ~\cite{studentbehavioranalytics} that talks about using higher education big data to evaluate student behavior. This paper repeats a similar claim to ~\cite{Hassna02102023} above in that higher education is missing work in big data.

    \cite{Okewu10112021} explores using artificial neural networks for higher education data mining. It provides a review of studies from 2010 through 2018 and related to education data mining focused on adaptive learning. Adaptive learning is the use of learner data to customize learning dynamically to those learners. So, we are looking at yet another narrower use case for a specific type of higher education learner data. This paper again mirrors some of the sentiments from earlier papers in that it hints that higher education data usage, and big data usage in particular, should be further along than where it is currently at (as of the paper's publishing in 2021), closer to where we see big data usage in other domains, but it is not.

    \cite{10.3389/fpsyg.2021.698490} investigates education data mining techniques for student performance prediction. There is not a ton more to add here aside from it being another narrower use case looking at student-level data, not the larger institution-level data that we explore in this project, but it is yet another example to learn from in terms of research using higher education data.

    \cite{ALDOWAH201913} digs into education data mining, similar to the last paper, but this time focused on learning analytics. It reviews studies in different domains related to learning analytics. The key takeaway is that it is yet another study that looks into different outcomes within specific areas of higher education data, but it is not close enough to what we are exploring in this project to lean on.

    Finally, ~\cite{https://doi.org/10.1155/2022/8924028} looks at predicting higher education student performance, typically at graduation, and it evaluates different predictive methods. Again, this is interesting for reviewing some current literature, but it will not apply to the work we do in this project.

    We will also include a quick review of a few papers related to synthetic data and synthetic tabular data generative modeling. We will utilize synthetic data, but we are not building generative models from scratch, so we will not dwell on the deep learning implementation details of these papers.

    \cite{decristofaro2024syntheticdatamethodsuse} provides an overview of synthetic data that is helpful for understanding the need for, uses of, and issues with synthetic data. We will use a production library for synthetic modeling, so we will not address these concerns directly except as parameters to the model(s) we decide to use in the pipeline, but it is good to understand the kinds of trade-offs that are inherent in generative modeling for synthetic tabular data.

    \cite{DBLP:journals/corr/abs-1811-11264} introduces table GAN (TGAN), a GAN-based approach to generating synthetic tabular data. This model uses recurrent neural networks. Each of the generative modeling papers I reviewed have extensive coverage of how to handle concerns in the data. One interesting one to highlight, among a few, from this paper and demonstrating the type of complexity that goes into this type of modeling is the focus on multimodal continuous data. This is continuous data that has more than one value that stands out as more represented in the data, showing up as more than one peak when you plot a histogram of the data. To get around this, the researchers use Gaussian kernel density estimation (KDE) to estimate the distribution of the data. Each KDE can model one mode of the data. We then use Gaussian mixture methods (GMMs) to sample from more than one distribution --- each distribution coming from one of those built with the KDEs --- to recreate the actual distribution. We then sample from this GMM when we are generating a value for that feature, and this gets baked into the modeling, training, and generation processes. This is just one example to demonstrate why we are veering away from building our own generative models in this project.

    \cite{DBLP:journals/corr/abs-1907-00503}, by the same authors as the previous TGAN paper, provides more consideration for nuances and potential issues in the data along with adding a deep conditional GAN (DCGAN) as part of their new conditional tabular GAN (CTGAN) model. DCGAN generally is a GAN architecture that uses convolutional neural networks (CNNs) in both the generator and discriminator. One example of extra data considerations is that CTGAN handles severe imbalance in categorical features. We will not worry about the detail of this model either, but it is potentially one that we will use with the modeling library we introduce next.

    \cite{sdv} is our project of choice for generative modeling. It is produced by the authors of the previous two papers above. Similar to those two papers, we will not dig too far into the workings of the libraries in this project. We mostly need a solution that can plug into the synthetic tabular data generative modeling step(s) in the pipeline. Note that both TGAN and CTGAN are dedicated repositories in the Synthetic Data Vault project. There is another main library --- SDV --- that provides easier-to-use versions of the models. There is also SDGym, a synthetic data evaluation library that we can lean on to determine how well our generative modeling is doing.

    With a sweep through some research papers out of the way, I did not find any papers directly applicable to what we are looking at in this project. So, the contribution that we provide is to start a discussion around data mining for IPEDS data along with generative modeling for IPEDS data. We build on the ideas in the papers above --- the general approaches to analysis in the earlier papers, and the use of synthetic tabular data as described in the latter papers.

\section{Proposed Work}

    The general approach for the work in this project is to recreate a full data pipeline on a smaller scale to demonstrate what we could build out further in a production system. We use Python scripts to simulate pulling data from application programming interfaces (APIs), run databases in Docker --- including a very small scale warehouse --- to stand in for real-world database usage, and leverage Jupyter Notebooks for exploratory data analysis (EDA) and modeling, including our core focus of generative modeling to create synthetic tabular data. We hold off on saving the final tuned versions of models and putting them into automated pipelines, but that would be an extension that we would include in a production system. The main idea is to have all the pieces that we would normally tap into in a regular-enough end-to-end data system to provide a foundation from which to expand on later.

    You can view the source code at \href{https://github.com/jhleakakos/msds-data-mining-project}{https://github.com/jhleakakos/msds-data-mining-project}.

    \subsection{Pipeline Walkthrough}
        We start by using a Python script as a placeholder for some sort of orchestration system. Separate orchestration software would be overkill right now, so we just import functions related to pulling data from the IPEDS website and call those from a dedicated orchestration script, but we would at some point possibly look into something like Apache Airflow to handle orchestration.

        IPEDS data already comes in a relatively relational format, and that lends itself well to loading it directly into a relational database serving as a data lake. We often work with databases in real-world projects, so it makes sense to include one here. We run a PostgreSQL database locally in Docker. Because Postgres connections are database-specific, we use one shared databse for this layer of the data lake and the coming data warehouse layer, using schemas to separate out the data lake from the data warehouse. This allows us to use structured query language (SQL) scripts to move data between the data lake and data warehouse without needing to use any other tooling, simplifying the handling of data in the database.

        We pull a subset of data out of the data lake and explore it in a Jupyter Notebook. The main goal here is to understand the already trimmed IPEDS data and determine where we want to start in terms of what gets moved into the warehouse. IPEDS data is vast and often difficult to work with, so it is important at this stage to narrow down the scope of the data to something reasonable and that allows us to keep our focus on the coming modeling. One big area of focus is on looking at null or missing flags in the IPEDS data and deciding how to handle those when we move data into the warehouse.

        We then use SQL script to move data from the data lake into a very rudimentary version of a dimensional model in the data warehouse, leveraging a standard star schema. Again, this is super small compared to what you would normally see in a real warehouse, but it lets us start to explore different ways of working with IPEDS data on a small scale before ramping up size and complexity in future itertations. As stated above, we use schemas in a shared database to separate out the data lake and data warehouse.

        Note that we do all the database work manually in SQL scripts. This is one of the first areas for improvement in future iterations. An easy start would be to store the current SQL scripts as stored procedures and call them either from an orchestration script or from update or insert triggers in the database. I went back and forth on adding some of these in, but, for the focus of the current project, it felt more important to keep an eye on the modeling instead of adding further complexity into the database layers. But this is probably the next improvement I would make, along with the improvement at the end of modeling mentioned later.

        Next we pull data out of the warehouse for modeling. We do another pass of EDA to explore how quirks in the data may affect generative modeling. We do see some issues and will speak about those in evaluation later.

        We then start in on the main focus of the project --- generative modeling to create new tabular synthetic data that can stand in for or supplement IPEDS data. As stated earlier, this type of generative modeling is notoriously difficult, so, since the focus of this project is not on building an optimized generative model but instead on generating new IPEDS data, we leverage already existing libraries for the generative modeling --- the Synthetic Data Vault project and its CTGAN generative model in particular. I had decided early on to use CTGAN for generative modeling since I spent some time exploring the associated CTGAN paper in a previous project, but there are a number of models and ways to experiment with them, and I tested each of those out before pivoting to working with CTGAN.

        While the notebook for generative modeling is not too long in its current state, much of my time outside of understanding the IPEDS data has gone into experimenting with the generative modeling. The SDV library's APIs are relatively straightforward to work with, but there are many hyperparameters to play around with. There are also decisions about dropping some data features to facilitate quicker and more accurate generative modeling, dropping categorical features with too many categories in particular.

        I used feedback from generative modeling steps and from my understanding of what could negatively affect generative modeling to help further trim down the feature space of the IPEDS data used for modeling. For example, we cut out some of the categorical variables that would be interesting to explore in terms of meaning --- race/ethnicity and gender as two major groups of categorical variables --- because these increase the feature space and copmlexity of inter-feature relationships in a way that can abstract from the needs of the first pass of this project --- feeling like we have landed on a decision about if using our approach to synthetic IPEDS data generation is feasible.

        While working on generative modeling, we also use the SDMetrics library to evaluate the generative modeling process and outputs. For example, we are able to provide a summary number for the quality of the synthetic data in terms of how close it is to the real data, we are able to visualize the GAN losses when training the CTGAN model, and we are able to visualize a given feature in both the real and synthetic data or pairs of features to see different angles into how similar or dissimilar the synthetic data is to or from the real data.

        Once we have our synthetic dataset, we move on to the final stages of the project. I am still working through this part of the project, but the plan is to spend some time testing out different modeling techniques on the real data, the synthetic data, and a combination of both datasets to see if we get similar outputs on all three. We have two numeric features that lend themselves to regression. We could pick some of the categorical features and experiment with classifications. And we could decide to branch out to something more complex such as clustering or association mining.

        There are two competing outputs in this project. The focus is on creating synthetic data, but we do not want to lose sight of making sure that the data we synthesize could lead to useful insights. I have been exploring synthetic data in personal and school projects as well as using it at work, so it is fresh on my mind, and I would really like to include it here, but it is in tension with what we might see in a standard data mining pipeline. We will figure out the balance for the next report update.

    \subsection{IPEDS Data}
        Note: you can find a file and field listing of the IPEDS data we use in the appendix, and you can find a more detailed summary of the IPEDS domains we work with in data/data\_notes.md in the GitHub repository.

        IPEDS has a ton of data available, so our first major task is to narrow that down to what we want to explore in this project. The approach we take is to pull in a subset of the three domains of IPEDS data we identified earlier: institutional characteristics, enrollments, and completions. There are many trade-offs in terms of pulling in more data or trimming down even to where we do.

        My primary worry with IPEDS data is its complexity. Two examples are as follows. First, a number of files in the domains we explore have more than 100 features, and there are many more files across domains that we do not explore, so there are concerns related to the size of the data. Second, we have similar features across files that are more complicated to combine or compare than they may seem at first. For example, we remove race/ethnicity and gender disaggregates in order to simplify overall count data, but these are important features for answering research questions.

        NCES provides preprocessing before making IPEDS data files available, including tasks such as imputation for missing values, creation of new features for estimated values to compare to reported values, and more. We will not worry about these initial preprocessing steps for this iteration of the project.

        The more data we include now, the harder the first pass of modeling will be, so we take the strategy of building out a full pipeline with a small subset of data in such a way that it can be expanded later without too much extra effort. Since our primary goal is generating synthetic data, we are less worried about finding the most impactful IPEDS features in terms of meaning in this iteration, giving us some flexibility for the modeling dataset that we choose for now.

        These are the three IPEDS domains that we work with:

        \begin{itemize}
            \item Institutional characteristics are the foundation of IPEDS data, providing information related to higher education institutions.
            \item Completions look at students who complete degree or nondegree credentials. This differs from graduation rates that measure students who receive degrees and exclude students who attain nondegree credentials, certificates being a common nondegree credential.
            \item 12-month enrollment data looks at unduplicated enrollment counts for students who start anytime in a 12-month period and including both degree- and nondegree-seeking students. IPEDS also has fall enrollment data, but we choose the wider timeframe. Since we include completions, a broader scope than graduations, we capture students who take a wider range of programs. 12-month enrollment will better capture nondegree students in particular.
        \end{itemize}

        We choose these three domains since they give us a basic count of students coming into higher education institutions each year and the number of completions --- a proxy for a type of output --- within that same year.

        IPEDS includes other domains of data that we do not use, including:

        \begin{itemize}
            \item Cost
            \item Financial aid
            \item Finance
            \item Admissions
            \item Outcome Measures
            \item Graduation rates: students who graduate with a degree within 150\% of the estimated time for program completion
            \item Graduation rates 200: students who graduate with a degree within 200\% of the estimated time for program completion
            \item Fall enrollment: students who enroll in the fall, as compared to the 12-month enrollment we use where students enroll at any point during a 12-month period
        \end{itemize}

        Each of these domains has multiple data files, similar to the three domains we work with. The numbers of questions we can ask of IPEDS data multiplies when we pull in these other domains, but we will save those questions for future iterations of this project.

    \subsubsection{Data Lake and Data Warehouse}
        We will take a moment to talk about some aspects of the database work. We use a database to mimic what we would likely encounter in real-world mining pipelines.

        We use two database layers: a data lake and a data warehouse, though the warehouse is a bit of a stretch in terms of the scope we have for it.

        We narrow down the entirety of the data in the three chosen IPEDS domain before loading that subset into the data lake. This narrowing focuses on pulling out counts that lend themselves to a first pass of generative modeling and removeing features or groupings of features that have a large number of categories. We identify counts across any of the raw files along with disaggregates that appear manageable for the first pass of the project. The primary change from what we outline in the appendix compared to what we load into the data lake is that we remove race/ethnicity and gender disaggregates.

        After the first pass of EDA, we need to add lookup tables for the categorical features that we choose to keep in the warehouse. The IPEDS dictionaries do not make this task as easy as it should be. Some features get dropped between the data lake and data warehouse due to lack of clear mappings for categorical values, even after searching for those on the IPEDS and NCES websites. Some of the mappings in the data dictionaries are clear, but others do not provide values for the descriptions that they list, so we do our best to try and figure out the mappings.

        One example is for boolean features. Some features use 1s and 2s in the original data while other features use 1s and 3s. We use the initial pass of EDA to determine which category is more prevalent and use a judgement call to map that to what we believe should be most prevalent according to general understanding of higher education data.

        In other cases, we need to use the IPEDS data explorer tool (\href{https://nces.ed.gov/ipeds/use-the-data}{https://nces.ed.gov/ipeds/use-the-data}) to check what values IPEDS has attributed to given institutions. For example, the raw data uses numbers for categorical values. When we are unclear if an institution is public, nonprofit private, or for-profit private, we can look that institution up on the IPEDS data explorer, check what value IPEDS has attributed to that institution, and then compare that description to the numeric value we find in the raw data. We repeat this for each categorical feature, building up mappings that we can add as lookup tables.

        We then perform the second reduction in data, primarily removing a number of enrollment and completion count categories and totals, simplifying the data further and starting to zoom in on a few numeric features that we can use for regression during modeling. We map this data into a dimensional-ish model in the warehouse. This is mostly meant as a proof of concept, and it mirrors a setup we could likely run into in real-world applications.

        We limit the numeric aggregates we use in the warehouse, only focusing on those that we want to synthesize during generative modeling. We also return to the IPEDS data explorer tool to sanity check the totals we get for enrollments and completions. It is not immediately clear the best way to group across the data from each of the tables in the data lake, mostly due to the issue of unclear categories for categorical features mentioned in the previous paragraph. So we test out different aggregations and compare the sums to what we see in the IPEDS data explorer too. Our counts end up higher than what we see in that tool, but we end up in a reasonable ballpark that lets us finish up the warehouse for this iteration of the project. For example, the top 10 largest institutions by enrollment in our warehouse match those that we see in external sources.

        We model the dimension table for institutional characteristics as a slowly changing dimension (SCD) since we expect that some of the values in this table could change for a given institution, but those changes are unlikely to be common or frequent. We expect enrollment and completion counts to change between years, but we do not typically expect values describing insitutions to change. We will not need to leverage this SCD in this pass of the project since we stick with only working with 2023 data, but we will see some changes when we start adding in more data.

        Note that the appendix has details on the features that we work with at the data lake and data warehouse steps.

    \subsubsection{Modeling and Analysis}
        Since this project is focused on generating synthetic data, we end up shifting some of the scope that would be given to typical analysis to focus more on evaluating the generative modeling and its synthetic data output. I do have a list of questions related to feature importance in terms of enrollment and completion numbers as well as a range of classification and clustering experiments that I planned on testing out. But, the generative modeling, while easy enough in a technical sense using the libraries we take advantage of, ended up taking a lot longer in terms of understanding what was going on and how to responsibly impelement it in the pipeline.

        In terms of generative modeling, we end up generating a smaller synthetic dataset that is focused on a handful of categorical features related to institutional characteristics along with matching enrollment and completion numbers. I will raise some questions about this approach later in the discussion section, but, for this pass of the project, it allows us to double the amount of data we have in the real dataset, and, as we will see in the evaluation section, the synthetic data is sufficiently similar to the real data to either substitute for or supplement that real data.

        As a reminder, we are abstracting many of the details of generative modeling since that could be an entire project to itself, though we will call out how the data we work with could affect the generative modeling step.

        Once we finish up generative modeling, we do test out linear regression to help evaluate the quality of the synthetic data. We start by confirming that there is enough of a linear relationship between the inputs and outputs to continue using a linear model. We then pull out a test set from the real data to check the trained linear models.

        Next, we have two sets of regressions focusing on enrollment and completions. We split each of these two into three different training sets: the real data, the synthetic data, and a combination of the real and synthetic data. This leaves us with six total models to evaluate. We use mean squared error (MSE) and adjusted r-squared ($R^2_a$) to evaluate each model. The focus here is on checking if we see similar values for each of these metrics across all three training sets for a given model. If we do see this, then we have some evidence that the synthetic data is resulting in similar evaluative outputs as the real data, either on its own or in combination with the real data.

\section{Evaluation}
    Due to scope and time, the evaluations in this iteration of the project focus on determining how good our synthetic data is. The original intention was to do through this and then do a full extra round of analysis on top of the data, including the synthetic data, to glean further insights, but we instead give more time and space to generative modeling and evaulation of those models and their outputs, sticking closer to the theme of the project.

    We leverage evaluation metrics in the Synthetic Data Vault Project to evaluate the synthetic data, including determining how similar the synthetic data is to the real data. Earlier we spoke of per-attribute statistics and machine learning score. These two categories capture most of what we use to evaluate the synthetic data.

    All of the metrics here aside from machine learning score come from SDMetrics, a library in the Synthetic Data Vault Project. The summaries of how the metrics here work are informed by the SDMetrics documentation.

    We start by checking out the GAN losses when training the CTGAN model.

    \begin{figure}[h]
        \centering
        \caption{GAN losses while training CTGAN model}
        \Description{Line plot of generator and discriminator losses during CTGAN training.}
        \includegraphics[width=\linewidth]{gan_losses}
    \end{figure}

    We see expected progression for the generator and discriminator loss. The discriminator loss stays relatively flat while the generator loss decreases. GAN training is notoriously difficulty, including for our use case, so it is promising that we see expected losses in a relatively short time. The final CTGAN we train finishes training in under a minute.

    SDMetrics includes a diagnostic report that checks the validity of the synthetic data. This checks that primary key fields are unique, that numeric and datetime fields fall within the minimum and maximum of the range for that field in the real data, and that categorical features contain the same categories as in the real data (\href{https://docs.sdv.dev/sdmetrics/reports/diagnostic-report/whats-included}{https://docs.sdv.dev/sdmetrics/reports/diagnostic-report/whats-included}). The diagnostic report returns 100\%, indicating that our synthetic data is valid according to these conditions.

    Next we check out the SDMetrics quality report. This report checks how well the synthetic data matches the statistical properties found in the real data. These include the per-attribute comparisons we spoke about earlier. Per-attribute comparisons check the same feature in the real and synthetic datasets, calculate metrics such as the mean, median, or standard deviation, and measure how similar these are between the dataset.

    \begin{table}
        \caption{SDMetrics Quality Report}
        \begin{tabular}{ll}
            \toprule
            Property & Score\\
            \midrule
            Column Shapes & 0.872 \\
            Column Pair Trends & 0.870 \\
            Overall Score & 0.871 \\
            \bottomrule
        \end{tabular}
    \end{table}

    The quality report includes two primary metrics: column shapes and column pair trends (table 1). Column shapes performs the per-attribute metric from above. For categorical features, column shapes compares the probability of each category in a given feature in the real and synthetic data and compares them (\href{https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/tvcomplement}{https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/tvcomplement}). For numeric features, column shapes compares the cumulative distribution function (CDF) for the feature in the real and synthetic data and finds the maximum distance between them (\href{https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/kscomplement}{https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/kscomplement}). With the way these are calculated, higher numbers are better, and scores closer to 100\% mean the synthetic data is more statistically similar to the real data.

    \begin{table}
        \caption{SDMetrics Column Shapes}
        \begin{tabular}{lll}
            \toprule
            Column & Metric & Score \\
            \midrule
            year & TVComplement & 1.000 \\
            state\_abbr & TVComplement & 0.882 \\
            bea\_region & TVComplement & 0.897 \\
            highest\_level & TVComplement & 0.938 \\
            is\_degree\_offering & TVComplement & 0.945 \\
            is\_hbcu & TVComplement & 0.973 \\
            is\_tribal\_institution & TVComplement & 0.974 \\
            geographic\_status & TVComplement & 0.892 \\
            date\_closed & KSComplement & 0.167 \\
            institutional\_category & TVComplement & 0.945 \\
            control\_affiliation & TVComplement & 0.906 \\
            enrollment & KSComplement & 0.887 \\
            completions\_number\_students & KSComplement & 0.932 \\
            \bottomrule
        \end{tabular}
    \end{table}

    \begin{figure}[h]
        \centering
        \caption{Column Shapes Scores}
        \Description{Bar plots of SDMetrics column shapes scores}
        \includegraphics[width=\linewidth]{column_shapes}
    \end{figure}

    Looking at table 2 and figure 2, we see some outliers. year gives perfect statistical similarity since every row has the same value at this point. Later iterations can load more than one year of data, so we want to keep this field around. date\_closed is mostly null, so that is likely why this field is so off --- there is not enough information in the real data for the model to learn meaningful patterns. The remaining features show more similarity between the real and syntethetic features indicated by their scores above 0.5. We do not want scores to be too close to 1.0 since that may result in a situation where the synthetic data is too real and can reveal confidential information from the real data. We end up dropping year and date\_closed for later modeling, so we are not worried about these features for now, and the remaining features look good.

    \begin{figure}[h]
        \centering
        \caption{Column Shapes Plot for bea\_region}
        \Description{Bar plot of SDMetrics column shape for bea\_region}
        \includegraphics[width=\linewidth]{column_plot_bea_region}
    \end{figure}

    \begin{figure}[h]
        \centering
        \caption{Column Shapes Plot for enrollment}
        \Description{Line plot of SDMetrics column shape for enrollment}
        \includegraphics[width=\linewidth]{column_plot_enrollment}
    \end{figure}

    SDMetrics also evaluates column pairs between the real and synthetic data. These are not as informative for our current focus, though we would want to return to these when evaluating machine learning score, a metric we explain next. But first, here are two sample column pair plots.

    \begin{figure}[h]
        \centering
        \caption{Column Pair Plot for enrollment and completions}
        \Description{Scatter plot of SDMetrics column pair relationship for enrollment and completions}
        \includegraphics[width=\linewidth]{column_pair_plot_enrollment_completion}
    \end{figure}

    \begin{figure}[h]
        \centering
        \caption{Column Pair Plot for is\_degree\_offering and completions}
        \Description{Box plot of SDMetrics column pair relationship for is\_degree\_offering and completions}
        \includegraphics[width=\linewidth]{column_pair_plot_is_degree_offering_completion}
    \end{figure}

    Finishing out per-attribute metrics, SDMetrics can compute the statistical similarity of the mean, median, and standard deviation of a feature across the real and synthetic data, measure the absolute value of the difference between one of these metrics across the datasets and then scaled by the range of the real data for that feature (\href{https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/statisticsimilarity}{https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/statisticsimilarity}).

    Next we finish up with machine learning score, training regressors on the real data, synthetic data, and a combination of both and comparing the performance of them with one another. If we find the MSE or $R^2_a$ is similar between the three training sets, then we have some evidence that the synthetic data is similar enough to the real data to stand in for it or to be combined with it without reducing model performance.

    \begin{table}
        \caption{Machine Learning Score}
        \begin{tabular}{llll}
            \toprule
            Metric & Training Set & Target & Value \\
            \midrule
            MSE & Real & Enrollment & 101,200,325.55 \\
            MSE & Synthetic & Enrollment & 111,006,681.53 \\
            MSE & Combined & Enrollment & 101,978,162.27 \\
            $R^2_a$ & Real & Enrollment & 0.19 \\
            $R^2_a$ & Synthetic & Enrollment & 0.11 \\
            $R^2_a$ & Combined & Enrollment & 0.19 \\
            MSE & Real & Completions & 116,066,105.90 \\
            MSE & Synthetic & Completions & 133,327,873.83 \\
            MSE & Combined & Completions & 129,922,536.22 \\
            $R^2_a$ & Real & Completions & -0.01 \\
            $R^2_a$ & Synthetic & Completions & -0.07 \\
            $R^2_a$ & Combined & Completions & -0.04 \\
            \bottomrule
        \end{tabular}
    \end{table}

    The values for each of the machine learning score metrics are pretty similar, hinting that the synthetic data is of pretty good quality. These are not necessarily the best linear regression models in terms of predicting enrollment or completion numbers, but the fact that the MSE and $R^2_a$ values are similar across the real, synthetic, and combined datasets means the generative modeling did its job.

    We do have some skewed numeric features and imbalanced categorical features in the real data. CTGAN has techniques to handle these, and it looks like those were able to handle these aspects of the real data without trouble. We also dropped some features with a large number of categories since they were slowing down generative modeling without providing much upside.

    In future iterations, I would like to include a couple of changes. First is adding a new evaluation metric where we predict each feature in a dataset with all the other features, repeating this across the real, synthetic, and combined datasets. If we find the predictive power in all three datasets are similar, then that can give us further evidence that the generative modeling is producing high-quality synthetic data.

    At this stage, we have run successful generative modeling on our subset of IPEDS data. These metrics look promising, with the summary numbers falling into ranges that hint that the synthetic data is picking up on patterns in the real data without mirroring the real data so closely that we run into privacy concerns.

    I chose to descope any further modeling beyond these in order to keep the scope from ballooning. Experimenting with and evaluating the generative models and their synthetic data outputs took longer than expected, though the time was well spent. The generative modeling base that we have in this iteration of the project will serve us well when we look to expand use cases in the future.

\section{Discussion}
    My goal for this project was to stick within a 3-4 week limit, and I was able to stick within this timeframe. I tend to drag out projects, taking the opportunity to add increased complexity and explore new items I am curious about. But, while spending forever on projects is fun and educational, I do risk getting too comfortable with taking my time. I could benefit from descoping and focusing on getting smaller chunks working earlier and then deciding if it is worth expanding out from there.

    The first area in which to keep an eye on scope is with the data used in the project. We start by selecting three IPEDS domains out of many. We then systematically trim the datasets down in stages, as described earlier. By the time we reach modeling, we have a dataset with 14 features. Two of those are numeric targets we can use for regression. We could also put some boundary cutoffs for those fields to test out classification. We have a couple of more likely features in terms of explanatory power, and we have a few extras to test out in terms of importance. Keeping the dataset smaller helps keep the modeling stages more focused.

    I also selected data that is complex but familiar. I have worked in higher education for years, so, while IPEDS data is itself complicated, it helps that I am used to using higher education data, again helping to keep the scope manageable. There are a lot of interesting higher education questions that got dropped while trimming the data down, but it will be feasible to add those back in in future iterations based on the design of the system.

    So, I ended up leveraging one of my backup plans in that I needed to descope the range of IPEDS included in the project. Three separate times.

    Next is the use of the database. This is a personal thing for me, but I am more familiar working in databases over the years, so doing work in the database simplifies the amount of time needed for me to set up a production-esque system. We also run into databases everything in real-world systems, so a project that mirrors a production pipeline, even if on a very small scale, must have database tech included in it, in my experience.

    Finally, a big area of scope reduction is at the end of the pipeline. I originally intended to implement both generative modeling and analytical modeling. Generative modeling is key to the focus of this project, but I was unclear if the project would feel incomplete if it did not have extra layers of analytical modeling as well. It was not until this final update for the project and deliverables that I decided to stick with the generative modeling and leave the analytical modeling until a future expansion of the project. Since I decided to explore generative modeling early in the planning stages, that took priority over analytical modeling, and the generative modeling itself took a significant chunk of time to familiarize myself with, experiment with, and implement. To add extra analytical modeling at this stage feels like it would balloon the scope and pull the focus of the project elsewhere.

    Speaking of generative modeling, even after all the tinkering I did for this project, I still have some lingering questions. The first is where the generative modeling should happen. The approach in this project was to pull the analysis dataset out of the warehouse and synthesize more data mimicking that dataset. If we zoom out and think about a pipeline and warehouse that needs to be more general purpose, there is a case to synthesize data earlier, possibly even keeping real and synthetic fact and dimension tables in the warehouse. If the goal of the synthetic data is to distribrute a final dataset, then our approach here works well. If the goal is to incorporate synthetic data as a core piece of the warehouse, then we may want to synthesize data earlier.

    I am still on the fence about the right answer here. I think the location of generative modeling in this pass of the project is correct, but the big downside is that we are generating new data based on a very small and bespoke subset of the overall IPEDS data that we start with in this project, and that is already a big reduction in terms of the overall IPEDS data that is available across more domains. With my current understanding of generative modeling, we may want the modeling to be able to look at a wider range of IPEDS data when learning patterns to synthesize since we may drop some important relationships between features before we hit generative modeling in this project. We could generate one much larger dataset and then push that into the warehouse, possibly adding generative modeling as a step between the EDA after the data lake and before transforming data into the dimensional model. The trade-off though is that it feels strange to add generative modeling in too early, like we are building a warehouse and pipeline primarily to serve synthetic data, not real data. This is one of my biggest questions that is left at the end of this iteration of the project.

    Another design question that has become more prominent as the project progressed has been how individual institutions may be able to combine their student-level data with the system here. As stated earlier, while synthesizing institutional-level data is valuable, it is likely that synthesizing student-level data is where the bigger value lies. One approach I can think of is to modify the pipeline to read in common student-level data, model it in the data lake and warehouse, and then prepare for it in terms of generative modeling, wherever we put that modeling step. I do not want to derail the report here with a lot of theorizing about how to incorporate student-level data, but it is worth noting that this is, along with where to plop the generative modeling, the biggest question I am left with at the end of the project.

    The initial main challenge that I was looking out for was much higher complexity in the IPEDS data. To be fair, the IPEDS data took a good chunk of the time in this project to sort through, and, as stated earlier, I purposely narrowed down the data to try and address this challenge.

    The next challenge was around how much automation to bake into the project. Ideally, I wanted to build out a fully automated pipeline, but I ended up finding that the extra effort to implement this would not add to the value of this iteration of the project. Instead, I have some placeholders and plans for how to increase automation in future iterations. These primarily look like leveraging the orchestration script, adding database objects such as stored procedures and triggers, and building out some utility scripts that help to add in new data. For example, a we could built a utility script that reads a new raw data file, pulls the headers, creates an SQL script to create a new table, and push the data into the database, as one example. This is in contrast to building all this out manually as we do at the moment, and it is one example of where we could use such a script.

    In terms of ongoing challenges, I think the big one is keeping an eye on how to evaluate the synthetic data, especially if we start modifying the pipeline as described throughout this section. While I am comfortable with running evaluation metrics, interpreting them across different use cases will take a lot more experience. It would be good to put processes in place to track evaluation metrics over time and for different uses in order to improve the value and quality of the synthetic data. For example, we do not want synthetic data to 100\% match the real data, but we also do not want it to be so different that the conclusions you reach with the synthetic data are invalid.

    All-in-all, I am left with a number of questions and concerns at the end of this iteration of the project, a good sign that we accomplished some good work while also having room to enhance and expand the system going forward.

\section{Conclusion}
    To recap, we are using three domains of IPEDS data to test out generative modeling to produce synthetic tabular data, preparing for the reality where we have less and less higher education data in the coming years. We set up a small sample of a full ETL pipeline, send the IPEDS data through, generate synthetic data, and do some exploratory modeling on that data. This approach is probably over-engineered for the scope of this project, but it lays the groundwork for pulling in much more IPEDS data and running more complicated analyses in the future. I also leaned into the architecting portion of the project, focusing on system design rather than picking one area to prioritize above the rest, though we do lean into generative modeling as the new-ish piece that we incorporate here.

    Overall, we have a full pipeline with hooks in it for future improvement, successful implementation of generative modeling along with evaluative metrics that support that claim, the ability to increase the scope of IPEDS data we work with, and a range of other questions and enhancements we can dig into after this iteration of the project. Incorporating generative modeling did add a fresh aspect for me into this work, and I am glad I did it, though it meant that I had to descope other areas to accommodate the new modeling steps.

    The biggest findings have been some of what we expected to see: the IPEDS data contains a lot more complexity than what it looks like at first. We address this by descoping the data that we work with in this pass. In future iterations, we will incorporate much more IPEDS data. That will result in a more robust and complicated warehouse, generative modeling, and exploration of questions with the synthetic data. But, for now, the scope here feels healthy for a first iteration on a project of this sort. If I could go back and scope this project again, I would consider building two smaller projects and then combining them --- one project focused on IPEDS data and the other focused on generative modeling.

    One area to call out that I do not plan to have an answer for in this iteration of the project is how we would pivot from IPEDS institutional-level data to student-level data, the latter being the area where are likely to see the biggest drop-off in future years. When doing initial planning for this project, I expected that there would be a couple of obvious areas in which to pivot from IPEDS data to student-level data, but those areas have not revealed themselves. One issue is that student-level data by its nature of being scoped to individual students is not and should not be in large aggregated and publicly available data such as what we find on IPEDS, primarily for privacy concerns.

    Probably the best path forward to include student-level data would be to modify the pipeline to prepare IPEDS data in such a way that individual institutions can clone the pipeline and incorporate their specific student-level data as needed. This idea has actually become my main area of interest for next steps after completing this project. It would require building out more utility functionality that lets users customize specifics in the pipeline. For instance, we could build a series of scripts that create separate SQL scripts or Python EDA scripts to incorporate new data, something akin to building a format for configuration that the system could be responsive to. But that would pivot us quite a bit away from the focus on generative modeling here, so we will leave it for a future iteration.

    In terms of the synthetic data itself, so far it appears to work as desired, allowing us to work with new data that is similar to the statistical trends we see in the real data. I have pulled in some of my higher education data experience when working with this batch of IPEDS data, but a first next step in a future iteration of the project would be to highlight areas where there are known difficulties and confusion with higher education data and exploring how the generative modeling handles those. And, as said above, the pipeline and approach here could use thorough testing and enhancement related to incorporating a much wider range of IPEDS data, a more complicated warehouse, and improved scope and usage of generative modeling.

\bibliographystyle{ACM-Reference-Format}
\bibliography{msds_data_mining_project_report}

\appendix

\section{IPEDS Fields}
    The summaries for files and fields here come from each file's associated data dictionary. Note that the biggest difference between the list here and the larger summary in data/data\_notes.md in the source repository is that we drop gender and race/ethnicity disaggregates here.

    \subsection{Data from Raw Files}

        Domain: Institutional Characteristics

        HDyyyy: directory information for institutions

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item INSTNM: institution name
            \item STABBR: state abbreviation
            \item OBEREG: Bureau of Economic Analysis regions, grouped by homogeneity of states based on economic, demographic, social, and cultural characteristics
            \item HLOFFER: highest level of credential offered
            \item DEGGRANT: does the institution offer degrees (as compared to non-degree credentials)
            \item HBCU: is the institution an HBCU
            \item TRIBAL: is this a tribal institution
            \item LOCALE: geographic status of school
            \item CLOSEDAT: date institution closed
            \item CYACTIVE: is institution active in current year
            \item INSTCAT: category of institution regarding credentials offered
            \item C21BASIC: alternative to Carnegie Classification and last updated in 2021
            \item INSTSIZE: enrollment size in 2019
            \item CBSA: core based statistical area
        \end{itemize}

        ICyyyy: program and award level offerings, control, and affiliation of institutions

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item CNTLAFFI: institution control or affiliation (public, private non-profit, etc)
            \item OPENADMP: open admission policy
        \end{itemize}

        ICyyyy\_PY: student charges by program

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item PRGMOFR: number of programs offered at institution
            \item CIPCODE1: CIP code of the largest program
            \item CIPCODE2: CIP code of the second largest program
            \item CIPCODE3: CIP code of the third largest program
            \item CIPCODE4: CIP code of the fourth largest program
            \item CIPCODE5: CIP code of the fifth largest program
            \item CIPCODE6: CIP code of the sixth largest program
        \end{itemize}

        Domain: Enrollments

        EFFYyyyy: unduplicated 12-month enrollment headcount

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item EFFYALEV: level, full- and part-time status, degree- or nondegree-seeking status, and year of study of student enrolled for credit
            \item EFFYLEV: level of study
            \item EFYTOTLT: total students enrolled for credit
        \end{itemize}

        EFFYyyyy\_DIST: unduplicated 12-month enrollment headcount by distance education status and level of student

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item EFFYDLEV: level and degree- or nondegree-seeking status of students enrolled for credit
            \item EFYDETOT: all students enrolled for credit (sum of next three columns)
            \item EFYDEEXC: students enrolled only in distance education courses
            \item EFYDESOM: students enrolled in at least one distance education course but not enrolled exclusively in distance education courses
            \item EFYDENON: students not enrolled in any distance education courses
        \end{itemize}

        EFFYyyyy\_HS: unduplicated 12-month enrollment headcount of dual credit students; subset of the nondegree and non-certificate-seeking students from EFFY2023

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item EFYTOTLT: total dual credit students
        \end{itemize}

        EFIAyyyy: instructional activity measured in total credit and/or contact hours delivered by institutions in 12-month period, used to derive 12-month full time equivalent (FTE) enrollments

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item EFTEUG: estimated FTE undergraduate enrollment
            \item EFTEGD: estimated FTE graduate enrollment
            \item FTEUG: reported FTE undergraduate enrollment
            \item FTEGD: reported FTE graduate enrollment
            \item FTEDPP: doctor's-professional practice FTE
        \end{itemize}

        Domain: Completions

        Cyyyy\_A: number of awards by type of program, level of award (degree or certificate), first or second major, by race/ethnicity, and by gender

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item CIPCODE: CIP code
            \item MAJORNUM: first or second major
            \item AWLEVEL: award level code
            \item CTOTALT: awards/degrees conferred to all recipients
        \end{itemize}

        Cyyyy\_B: number of students who completed any degree or certificate by race/ethnicity and gender

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item CSTOTLT: number of students receiving awards/degrees
        \end{itemize}

        Cyyyy\_C: number of students receiving a degree or certificate by level of award and race/ethnicity, gender, and age

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item AWLEVELC: award level
            \item CSTOTLT: number of students receiving awards/degrees
        \end{itemize}

        CyyyyDEP: number of programs offered by type of program, award level, and distance education status

        \begin{itemize}
            \item UNITID: unique institutional identifier
            \item CIPCODE: six-digit CIP code
            \item PTOTAL: number of programs offered across all award levels
            \item PTOTALDE: number of programs that can be completed entirely online across all award levels
            \item PASSOC: number of programs offered at associate's degree level
            \item PBACHL: number of programs offered at bachelor's degree level
            \item PMASTR: number of programs offered at master's degree level
            \item PDOCRS: number of programs offered at doctor's research/scholarship degree level
            \item PDOCPP: number of programs offered at doctor's professional practice degree level
            \item PDOCOT: number of programs offered at doctor's other degree level
            \item PCERT1A: number of programs offered at certificate level of less than 300 clock hours, 9 semester/trimester credit hours, or 13 quarter credit hours
            \item PCERT1B: number of programs offered at certificate level of 300-800 clock hours, 9-29 semester/trimester credit hours, or 13-44 quarter credit hours
            \item PCERT2: number of programs offered at certificate level of one year but less than two years
            \item PCERT4: number of programs offered at certificate level of two years but less than four years
            \item PPBACC: number of programs offered at postbaccalaureate certificate level
            \item PPMAST: number of programs offered at post-master's certificate level
        \end{itemize}

    \subsection{IPEDS Fields for Modeling}
        These are the fields we pull from the files above to create the modeling dataset. Note that we drop INSTNM and CBSA during generative modeling and later since these features have a ton of categories each, and all those categories wreak havoc on the CTGAN performance without providing enough meaning to justify the performance hit. For context, the CTGAN model takes more than 30 minutes to train with these features and under a minute without them.

        \begin{itemize}
            \item year: we snag this when reading in files and add it as needed throughout the warehouse
            \item HDyyyy.UNITID: unique institutional identifier
            \item HDyyyy.INSTNM: institution name (not using during actual modeling because of the number of unique categories)
            \item HDyyyy.STABBR: state abbreviation
            \item HDyyyy.OBEREG: Bureau of Economic Analysis regions, grouped by homogeneity of states based on economic, demographic, social, and cultural characteristics
            \item HDyyyy.HLOFFER: highest level of credential offered
            \item HDyyyy.DEGGRANT: does the institution offer degrees (as compared to non-degree credentials)
            \item HDyyyy.HBCU: is the institution an HBCU
            \item HDyyyy.TRIBAL: is this a tribal institution
            \item HDyyyy.LOCALE: geographic status of school
            \item HDyyyy.CLOSEDAT: date institution closed
            \item HDyyyy.INSTCAT: category of institution regarding credentials offered
            \item HDyyyy.CBSA: core based statistical area (not using during actual modeling because of the number of unique categories)
            \item ICyyyy.CNTLAFFI: institution control or affiliation (public, private non-profit, etc)
            \item EFFYyyyy.EFYTOTLT: total students enrolled for credit
            \item Cyyyy\_B.CSTOTLT: number of students receiving awards/degrees
        \end{itemize}

\end{document}
\endinput
