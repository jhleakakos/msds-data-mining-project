\documentclass[sigconf, authorversion, nonacm]{acmart}

\settopmatter{printacmref=false} % remove acm info that clutters things up for this project
\settopmatter{printfolios=true} % add page numbers

%\citestyle{acmauthoryear}

\usepackage{hyperref}

\begin{document}

\title{Higher Education Synthesized}
\subtitle{Extending Higher Education Data with Synthetic Data at the Onset of the Demographic Cliff}

\begin{abstract}
    American higher education is coming to an impasse. Colleges and universities have seen enrollments grow as larger generations of Americans made their way through postsecondary education. Schools increased their academic offerings and student life activities. Faculty and staff members filled the needs for supporting increasing student enrollments. But there was an issue hiding behind all of this growth: what happens when student enrollments systematically fall?

    The United States has seen declines in high school graduates and, therefore, reduced college applicants. Depending on who you listen to, this ranges from a demographic cliff ushering in an emergency in higher education to a slower decline in higher education enrollments that will play out over years. It is unclear what effect this will have on the near future of higher education. One expected consequence of these declines is that we will see smaller amounts of higher education data relative to previous years.

    This project focuses on synthesizing Integrated Postsecondary Education Data System data from the National Center for Education Statistics. This data covers a vast range of information about higher education, and, while we will not look at student-level data --- one of the key areas for synthetic data after the demographic cliff hits --- we will start by generating synthetic data based on subsets of institution-related data. The goal is to show how we might address the diminishing data after the demographic cliff in order to continue higher education data analysis that relies on larger datasets.
\end{abstract}

\maketitle

\section{Introduction}
    American higher education saw growing enrollments throughout the latter half of the 1900s and moving into the 2000s. This resulted in increases in the number of higher education institutions (HEIs) in the country, including public and private colleges and universities. These HEIs span from major research institutions down to small liberal arts schools. This has also resulted in increases in the number of faculty and staff to support growing enrollments. The general trend in higher education was growth.

    The issue is that this growth in enrollments was not to last. For years now, we have seen predicted and then realized declines in high school graduates along with the respective drop in higher education enrollments. Higher education's future was based on growth, but then the key aspect of that growth --- more students than in previous years --- fell away. Estimates for what comes next vary. Some see a demographic cliff and a looming emergency in which declining enrollments result in catastrophic cuts at and closures of HEIs. It is true that we have seen increasing numbers of HEI closures in recent years ~\cite{hechingercollegeclosures}. But others see the trend as a more gradual decline, giving higher education time to adapt and shrink accordingly. \cite{insidehighereddemographiccliff}

    There are many interesting research questions related to the future of higher education, appropriate and responsible responses to enrollment declines, and other aspects of this impending reality. There are also questions related to the shifting of costs from public funding to individual students, the shift in emphasis to higher education as workforce preparation, the modern publishing culture in higher education and how that affects the student experience, and more.

    But, this project will instead focus on a different aspect of declining higher education enrollments: reduced data. As student enrollments drop and HEIs close or shrink, we are likely to see reduced amounts of higher education data available for analysis, prediction, evaluation, and more. Not all analyses in higher education need larger amounts of data, but we want to explore how we could supplement real data with synthetic data in cases where we need more overall data. The risk is that, without sufficient data, we may not be able to support students and HEIs as well as we could with larger amounts of data.

    We will take a look at Integrated Postsecondary Education Data System (IPEDS) data from the National Center for Education Statistics (NCES), housed in the U.S. Department of Education's Institute of Education Sciences (IES). NCES gathers, prepares, and makes available data along with providing analysis on education data. IPEDS is a subset of NCES data that focuses on postsecondary or higher education data. IPEDS data is large, complex, and valuable, so it also lends itself well to data mining. \cite{ipeds}

    We will focus on a subset of IPEDS data related to institutions, enrollments, and graduation rates. For each of these three domains, IPEDS offers multiple files covering different information, and it provides these for multiple years. Our goal will be to identify one or more analysis or modeling datasets that we can pull out of a larger extract-transform-load (ETL) pipeline with multiple repeated steps. We will focus on pulling IPEDS data, loading it into a relational database in its raw form, preprocessing it in the database with a series of automated processes, storing the analysis or modeling datasets in the database, and then using those datasets to synthesize new data.

   To address the issue of the amount of data available to us, we will leverage generative modeling to produce synthetic tabular data. The goal is to build a system that can demonstrate how we might leverage synthetic higher education data to fill in datasets in the future as they potentially shrink due to the issues described above.

    We are focusing on institution-level data from IPEDS, but future iterations of this work could pivot to generating synthetic student-level data.

    Synthetic data is ``artificially generated [data] that resemble[s] the actual data -- more precisely, having similar statistical properties'' ~\cite[p. 1]{decristofaro2024syntheticdatamethodsuse}. If we generate synthetic IPEDS data that shares the same properties as the real data, then we can combine the real and synthetic data, or, alternatively, we can create an indefinite amount of synthetic data, to increase the amount of data we have to work with.

    Building and training models to generate synthetic tabular data -- tabular meaning data in table form with rows and columns -- is notoriously difficult. We will leverage existing models and research for the synthetic generation aspect of the pipeline in particular. We will lean on the research in the Modeling Tabular Data Using Conditional GAN paper ~\cite{DBLP:journals/corr/abs-1907-00503} and the associated Python libraries they make available at the Synthetic Data Vault Project ~\cite{sdv}.

    While we will not dive too deep into the working of the generative models, it can be useful to understand the complexity in these and why we are leveraging existing implementations. The research paper focuses on their conditional tabular generative adversarial network (CTGAN) approach to generative modeling. This paper provides some examples of the difficulty of modeling for tabular data. Section 3 speaks to some of these issues:

    \begin{itemize}
        \item Mixed data types: tabular data can have features with continuous, nominal, ordinal, binary, or other data. Generative modeling for these datasets require handling mixtures of data types in one model.
        \item Non-Gaussian distributions: continuous data can be non-Gaussian, so attempts to standardize or min-max continuous features can result in issues such as vanishing or exploding gradients.
        \item Multimodal distributions: continuous features may have more than one mode, resulting in the need to model more than one mode with approaches such as kernel density estimation and Gaussian mixture models.
        \item Imbalance in categories for categorical features: categories that have smaller representation may not be modeled as accurately as those with larger representation, resulting in the need for extra steps to handle this disparity. \cite{DBLP:journals/corr/abs-1907-00503}
    \end{itemize}

    We do not need to go into the inner workings of these issues. Instead, they are meant to provide a snippet of the difficulties we would need to handle if building our own generative models. In order to remain focused on the pipeline in this project, we will, as said above, leverage existing generative modeling libraries available through the Synthetic Data Vault Project ~\cite{sdv}.

    Finally, the ultimate goal is to glean insights from the IPEDS data, real or synthesized. The synthetic data will ideally give us the same overall insights as the real data. For this task, we can turn to traditional data mining techniques: classification, clustering, association mining, rule mining, or others. If we can determine trends now, we may be able to utilize synthetic data in the future with dwindling real data to confirm continued trends or to identify changes in those trends.

    To summarize, this project will work with IPEDS data with its focus on higher education and will explore how we can synthesize more IPEDS data to address potential future lack-of-data issues due to declining higher education enrollments, and we will use these data to tackle standard data mining questions related but focused on higher education data.

\section{Related Work}
    My literature review did not return as much related to data mining in higher education as I expected to see, and I was unable to find any research closely related to what we explore in this project. The bulk of what I found is related to different evaluation methods for student learning. Examples are looking at adaptive learning --- learning that dynamically adjusts to students based on different input metrics --- and performance prediction --- trying to determine how a student will end up performing by, for example, graduation. While these are interesting, they do not focus on the type of data that we see in IPEDS.

    Note that some of these papers were already referenced in the introduction above.

    To start with, ~\cite{sdp} by the Harvard Strategic Data Project (SDP) fellowship program speaking about the importance of analytics in higher education data. There are larger questions that this paper raises about why higher education appears behind other areas in terms of leveraging data and more advanced analytics.

    Another example of an interesting paper that I want to highlight is ~\cite{Hassna02102023} that explores utilizing big data and analytics for higher education data. The catch is that this paper is from 2022, and it is talking about introducing big data and analytics in higher education, mirroring some of what we see from the SDP paper above in hinting that higher education may be behind other areas in terms of utilizing data. But, during the literature review, I checked out big data and data mining and related terms in other domains, and I did not tend to see papers talking about how big data and analytics have yet to be realized in those domains.

    Next is a handful of papers characterizing what I mostly found: research into specific uses of analytics and mining to hone in on an aspect of student evaluation. Since our focus on this project is on institution-level information with IPEDS data, we are zooming out quite a bit from what these papers cover, but these are useful in terms of getting bearings for research on higher education data mining, though none of these papers are overly useful for the specific use case in this project.

    First is ~\cite{studentbehavioranalytics} that talks about using higher education big data to evaluate student behavior. This paper repeats a similar claim to ~\cite{Hassna02102023} above in that higher education is missing work in big data.

    \cite{Okewu10112021} explores using artificial neural networks for higher education data mining. It provides a review of studies from 2010 through 2018 and related to education data mining focused on adaptive learning. Adaptive learning is the use of learner data to customize learning dynamically to those learners. So, we are looking at yet another narrower use case for a specific type of higher education learner data. This paper again mirrors some of the sentiments from earlier papers in that it hints that higher education data usage, and big data usage in particular, should be further along than where it is currently at (as of the paper's publishing in 2021), closer to where we see big data usage in other domains, but it is not.

    \cite{10.3389/fpsyg.2021.698490} investigates education data mining techniques for student performance prediction. There is not a ton more to add here aside from it being another narrower use case looking at student-level data, not the larger institution-level data that we explore in this project, but it is yet another example to learn from in terms of research using higher education data.

    \cite{ALDOWAH201913} digs into education data mining, similar to the last paper, but this time focused on learning analytics. It reviews studies in different domains related to learning analytics. The key takeaway is that it is yet another study that looks into different outcomes within specific areas of higher education data, but it is not close enough to what we are exploring in this project to lean on.

    Finally, ~\cite{https://doi.org/10.1155/2022/8924028} looks at predicting higher education student performance, typically at graduation, and it evaluates different predictive methods. Again, this is interesting for reviewing some current literature, but it will not apply to the work we do in this project.

    We will also include a quick review of a few papers related to synthetic data and synthetic tabular data generative modeling. We will utilize synthetic data, but we are not building generative models from scratch, so we will not dwell on the deep learning implementation details of these papers.

    \cite{decristofaro2024syntheticdatamethodsuse} provides an overview of synthetic data that is helpful for understanding the need for, uses of, and issues with synthetic data. We will use a production library for synthetic modeling, so we will not address these concerns directly except as parameters to the model(s) we decide to use in the pipeline, but it is good to understand the kinds of trade-offs that are inherent in generative modeling for synthetic tabular data.

    \cite{DBLP:journals/corr/abs-1811-11264} introduces table GAN (TGAN), a GAN-based approach to generating synthetic tabular data. This model uses recurrent neural networks. Each of the generative modeling papers I reviewed have extensive coverage of how to handle concerns in the data. One interesting one to highlight, among a few, from this paper and demonstrating the type of complexity that goes into this type of modeling is the focus on multimodal continuous data. This is continuous data that has more than one value that stands out as more represented in the data, showing up as more than one peak when you plot a histogram of the data. To get around this, the researchers use Gaussian kernel density estimation (KDE) to estimate the distribution of the data. Each KDE can model one mode of the data. We then use Gaussian mixture methods (GMMs) to sample from more than one distribution -- each distribution coming from one of those built with the KDEs -- to recreate the actual distribution. We then sample from this GMM when we are generating a value for that feature, and this gets baked into the modeling, training, and generation processes. This is just one example to demonstrate why we are veering away from building our own generative models in this project.

    \cite{DBLP:journals/corr/abs-1907-00503}, by the same authors as the previous TGAN paper, provides more consideration for nuances and potential issues in the data along with adding a deep conditional GAN (DCGAN) as part of their new conditional tabular GAN (CTGAN) model. DCGAN generally is a GAN architecture that uses convolutional neural networks (CNNs) in both the generator and discriminator. One example of extra data considerations is that CTGAN handles severe imbalance in categorical features. We will not worry about the detail of this model either, but it is potentially one that we will use with the modeling library we introduce next.

    \cite{sdv} is our project of choice for generative modeling. It is produced by the authors of the previous two papers above. Similar to those two papers, we will not dig too far into the workings of the libraries in this project. We mostly need a solution that can plug into the synthetic tabular data generative modeling step(s) in the pipeline. Note that both TGAN and CTGAN are dedicated repositories in the Synthetic Data Vault project. There is another main library -- sdv -- that provides easier-to-use versions of the models. There is also SDGym, a synthetic data evaluation library that we can lean on to determine how well our generative modeling is doing.

    With a sweep through some research papers out of the way, I did not find any papers directly applicable to what we are looking at in this project. So, the contribution that we provide is to start a discussion around data mining for IPEDS data along with generative modeling for IPEDS data. We build on the ideas in the papers above -- the general approaches to analysis in the earlier papers, and the use of synthetic tabular data as described in the latter papers.

\section{Proposed Work}
    The general approach for the work in this project is to leverage Python scripts to move information around, a relational database running in Docker for any pieces of the pipeline that would normally use a database, and notebooks as needed for exploratory and visualization purposes.

    To snag the IPEDS data, we will use Python scripts to download the data from the IPEDS website and store it in a relational database, possibly storing the data on disk as a stepping stone. The database will run locally in Docker. IPEDS data already comes in a relatively relational form, so there may not be too much work for the initial load into the database. Much of the data we end up working with in the real world lives in a database, so this recreates that reality here.

    We may leverage the database for preprocessing. We would need to do some manual exploration first in Python, but, after that, we could look at shifting preprocessing into structured query language (SQL), particularly stored as insert triggers in the database. Right now, there is a question about how that would intersect with exploratory data analysis (EDA) that we would end up doing in Python, but we can figure out the specifics as we dig into the data. My thought here is that a production pipeline would be simpler if we could handle more of the preprocessing automatically in the database and not require that someone hook into separate Python scripts, whether automated or needing to be run manually.

    We will need to go through a full EDA process. This step will happen outside of the database, so we will look at Python scripts here. At first, this is likely easier with the read-eval-print loop (REPL) functionality of a notebook, though we do not want notebooks in our final pipeline until much closer to the end in production. We need the automation power in scripts instead of notebooks. We may also learn more from the EDA that we can bake into extra preprocessing, informing what is mentioned in the last paragraph.

    After this round of EDA and any further preprocessing we glean from it, we can then think about warehousing. We will shift the data back into the database, likely just matching the new form of the data to what we need for modeling, but we can decide later on regarding the form of the data in the warehouse. For the scope of this project, we will likely end up with a bespoke warehouse built to immediate modeling or analysis needs rather than a broader warehouse set up in something like a full star schema.

    The next step is to explore the outputs of the pipeline. It occurs to me that there are two competing outputs in this project: synthetic IPEDS data and standard evaluative findings. We will need to find a balance here, so we may limit the amount of regression, classification, or clustering we do to leave space for the synthetic data generative modeling, possibly demoing a smaller example of how we would glean insights out of the synthetic dataset using classification, for example, rather than adding more scope into that outputs after the generative modeling. The final outputs I have a sense will work well with the synthetic IPEDS data are regression for number of enrollments or degree completions, clustering to find groupings of institutions that we should keep an eye on, and classification for putting institutions into those groupings.

    I have been exploring synthetic data in personal and school projects as well as using it at work, so it is fresh on my mind, and I would really like to include it here, but, as said above, it is in tension with what we might see in a standard data mining pipeline. We will need to pull data out of the warehouse, import the synthetic data libraries, run the generative modeling, and then possibly load the synthetic data back into the database. This area has a bit of a question mark as to how much scope it will take up. I feel comfortable with how the generative modeling will work, but we will need to explore performance with IPEDS data in particular.

    The areas I am keeping an eye on here are the extra loop of preprocessing in the database, the inclusion of synthetic data, and narrowing down the final outputs. The ideas in this section are fine for now, but we need to make these more concrete as we begin digging into the data.

    \subsection{updates for proposed work}
        Is synthetic data generative modeling enough, or do we need to keep some extra modeling piece? If the latter, can it be more of a demo or POC than real analysis?

        Can reference technical details section in appendix to fill in from here

        Why synthesize the IPEDS data we use instead of student-level data, the area where we will see a real drop off in coming years? How are aggregates in IPEDS different from student-level data?

        handling different null flags or binary encodings even in same files (good use for slide, talking about data issues in general)

        talk about dimensional-ish modeling (good use for slide)

        goal of testing out mini data mart layer as well for whichever subset we want to synthesize

        call out emphasis on db work since we are mimicing a real-world mining pipeline that will likely rely heavily on db tech

        talk about slowly changing dimensions for institutional characteristics between years in particular and modeling decisions related to that

    \subsection{IPEDS Data}
        IPEDS has a ton of data available, so our first major task is to narrow that down to what we want to explore in this project. The approach we take is to pull in a subset of the three domains of IPEDS data we identified earlier: institutional characteristics, enrollments, and completions. There are a ton of trade-offs in terms of pulling in more data or trimming down even to where we do.

        My primary worry with IPEDS data is its complexity. Two examples are as follows. A number of files in the domains we explore have more than 100 features, and there are many more files across domains that we do not explore, so there are potential concerns related to the size of the data. Next, we have similar features across files. For example, we have breakdowns by race/ethnicity and gender and across different values for the primary attribute in files, and considering ways to combine these or compare across them are more complicated than they may seem at first.

        The more data we include now, the harder the first pass of modeling will be, so we take the strategy of building out a full pipeline with a small subset of data in such a way that it can be expanded later without too much extra effort. Further, since our primary goal is generating synthetic data, we are less worried about finding the most impactful IPEDS features in terms of meaning, so we have some flexibility for the dataset that we choose for now.

        \subsubsection{Chosen Domains}

            Institutional characteristics are the foundation of IPEDS data, providing information related to higher education institutions.

            Next, we use 12-month enrollment data, looking at unduplicated enrollment counts for students who start anytime in a 12-month period and including both degree- and nondegree-seeking students. IPEDS also have fall enrollment data, but we choose the wider timeframe. Since we also include completions instead of graduations, we capture students who take a wider range of programs. 12-month enrollment will better capture nondegree students in particular.

            Finally, we use completions, looking at students who complete degree or nondegree credentials. This differs from graduation rates that measure students who receive degrees and excluding students who attain nondegree credentials, certificates being a common nondegree credential.





            The goal for now is to continue to narrow down data within each of these domains, building towards a working dataset. The general idea is to have one row per institution, and we will need to group across rows in some files to make that happen. For example, some files have more than one row per institution, so we need to decide if it is appropriate to smoosh all of the rows per insititution together in those files and sum column values or if we need a more complicated approach. We figure these steps out doing the first round of EDA in the database.














        \subsubsection{What's Left from IPEDS}

            IPEDS includes other domains of data, including:

            \begin{itemize}
                \item Cost
                \item Financial aid
                \item Finance
                \item Admissions
                \item Outcome Measures
                \item Graduation rates: students who graduate with a degree within 150\% of the estimated time for program completion
                \item Graduation rates 200: students who graduate with a degree within 200\% of the estimated time for program completion
                \item Fall enrollment (as discussed above)
            \end{itemize}

            Each of these domains has multiple data files, similar to the three domains we are working with. The numbers of questions we can ask of IPEDS data multiplies when we pull in these other domains, but we will save those questions for future iterations of this project.

            These other domains may also pose questions related to synthesizing data for them.

        \subsubsection{Leftover Thoughts}

            NCES provides preprocessing before making IPEDS data files available, including tasks such as imputation for missing values, creation of new features for estimated values to compare to reported values, and more. We will not worry about these initial preprocessing steps for this iteration of the project.

            For each of the files we keep, we remove gender and race/ethnicity disaggregates. These are important features for answering meaning questions with IPEDS, but we remove them in order to simplify the analysis scope in this iteration of the project.

            For a more detailed summary of the IPEDS domains we work with, please read data/data\_notes.md in the GitHub repository.

\section{Evaluation}
    The first evaluations will capture how similar the synthetic data is to the real data. We will explore the evaluation metrics in the Synthetic Data Vault Project, and, on top of those metrics, a couple of extra candidates that stand out are per-attribute comparisons and machine learning score. Per-attribute comparisons check the same feature in the real and synthetic datasets, calculate metrics such as mean or standard deviation, and determine the quality of the synthetic data based on how close these measures are across the datasets. Machine learning score trains classifiers on the real and synthetic datasets and compares the performance of those models. For example, if we use both classifiers to check something like accuracy or F1 score on a held-out test set, we can evaluate how similar the synthetic data is to the real data based on comparing those scores.

    We can also look at more advanced evaluation metrics. One that has stood out to me predicting each feature in a dataset with all of the other features. If we find the predictive power in the real and synthetic datasets are similar, then, like above, that can give us a sense of how similar the datasets themselves are.

    There are any number of other evaluative metrics for synthetic data. We will explore what is available with the Synthetic Data Vault Project that we use and will clarify here as we learn more during experimentation. It is worth calling out here that we could expand the evaluation of the synthetic data itself, taking up more scope from later steps such as regression or classification or such. The risk of not spending enough time evaluating the synthetic data is that we may end up testing out further modeling on an ill-fitting synthetic dataset.

    Moving on from synthetic data, until we explore the IPEDS data further, I am not sure exactly what we will end up modeling or testing as output. But there are some general contenders that we could likely look at.

    For supervised modeling outputs, we can look at standard metrics such as accuracy, precision, F1 score, and more for classification problems. We can use a metric like mean squared error (MSE) or adjusted r-squared ($R^2_a$) to measure regression models. These evaluations give us firm numbers we can use to determine how well our modeling is doing. We may stick with supervised learning if we give more space to synthetic data evaluation since my experience is that the degree of difficulty jumps up when we pivot to unsupervised learning

    That said, I am actually more intrigued by seeing what we can find with unsupervised learning. Admittedly, the outputs for unsupervised learning can be more difficult to work with, especially at this early stage. Clustering is the first unsupervised modeling that comes to mind. I often think of clustering as a much more advanced version of EDA. We are not really sure what patterns exist in the data, so we supercharge standard EDA and see what patterns we uncover when leveraging machine learning. But we do not have a clean evaluation metric with clustering. We will need to interactively explore the clustering outputs, at least at first. At some point, we can determine a set number of clusters and pivot the clustering into a classification approach, but the more interesting piece here for me is the clustering itself.

    I am on the fence about leaning more into the synthetic data approach in which we would spend a lot more time evaluating the quality of the synthetic data. With synthetic data, I still would like to test out one of the analytical pieces at the end of the pipeline, but maybe we stick to something simpler like a clearer classification problem. If we instead drop synthetic data, then we have more room to tackle something like unsupervised learning and the complexity inherent in it.

    While I feel like there are loads of evaluative methods to explore, I am also interested in looking at some of the algorithms from the second data mining course. I have leveraged things like similarity scores in other projects, but I have not experimented with, for instance, FP growth, and it would be interesting to possibly pick a brand new algorithm to plop at the end of the pipeline and figure out what it means to evaluate that algorithm and what it reveals to us. But, similar to with unsupervised learning, my desire to explore synthetic data may mean leaving out some of the new algorithms from the second course for now. This would be an area to explore more in the next iteration of the project though.

\section{Discussion}
    My goal for this project is to stick within a 3-4 week limit, if possible. I tend to drag out projects, taking the opportunity to add increased complexity and explore new items I am curious about. But, while spending forever on projects is fun and educational, I do risk getting too comfortable with taking my time. I could benefit from descoping and focusing on getting smaller chunks working earlier and then deciding if it is worth expanding out from there.

    In that vein, I am hoping that this draft is an upper bound for the work in this project. Some areas where I would like to bound the scope a bit would be in the back-and-forth with the database earlier in the pipeline, how much to dig into synthetic data, how wide of a range of questions to explore in the data, and what types of modeling or pattern mining to use at the end of the pipeline. Since I have not started in on the technical work or deeper dives into the data yet, it feels okay to start out more expansively as is done here and look for the next draft to hone in on a more accurate scope.

    I did spend a bit of time looking through different public datasets, a number of those through NCES, ultimately landing on IPEDS, but I still need to spend a bit more time with the chosen IPEDS data. I have a sense the three domains of IPEDS data may get trimmed down to two domains. But gaining clarity on the data itself will take some time.

    The main challenge I see is that the data ends up being much more complicated to work with than expected, and that will take time to reveal itself, particularly in the understanding and preprocessing steps of the pipeline. One goal will be to get to those steps sooner to figure out what it will be like working with this data. The next big challenge I foresee is when we hit modeling, both the generative modeling for the synthetic data and whichever modeling we choose to employ on that synthetic data, though, as said above, the goal is for the latter to remain simpler in this project, leaving space to focus on the synthetic data itself.

    The general backup plan is to simplify the IPEDS dataset down quite a bit. We may even be able to get away with using just one domain of IPEDS data if needed. Since a major goal of this project is to demonstrate synthesizing IPEDS data, we may also only keep data that looks relatively clean to start with. We will also get a sense of which features in the data may be easier to model, again providing a way to simplify the scope to accommodate competing needs.

    To wrap up with an explicit acknowledgment of risks for my projects, the biggets worry is that the scope grows to an unruly size, so there are a number of comments in this report speaking to ways to keep that in check, spanning from stricter time scoping to simplifying the data to sticking with easier-to-implement modeling.

    \subsubsection{Warehousing}
    \subsubsection{Generative Modeling}
    \subsubsection{Data Analysis or Business Intelligence}

\section{Conclusion}
    There is not much to conclude at the moment. We will fill this in as we start doing the data work of the project. What I expect to see here is that we trimmed down the IPEDS data, scoped the use case for the project to be a bit narrower, spent more time explore synthetic data and its associated generative modeling, and picked a simpler overall output, likely sticking supervised modeling. For a surprise finish, we may end up deciding to pull some data out and run it through a separate business intelligence tool like Tableau with the idea that end users of the production pipeline may want to leverage tools such as that for their purposes, though this would only come into play if we find that the project turns out to be too quick and simple to get through.

    As for future iterations of the project, I expect that the biggest area of grows would be to incorporate more IPEDS data to answer a broader range of questions. I do not expect radical changes in the pipeline architecture in next iterations of the project, but I do expect additions and expansions in order to accommodate particular questions of interest inherent in the new data.

    \subsection{additions for conclusion}

    This approach is probably over-engineered for the scope of this project, but it lays the groundwork for pulling in much more IPEDS data and running more complicated analyses

\bibliographystyle{ACM-Reference-Format}
\bibliography{msds_data_mining_project_report}

\appendix

\section{Technical Details}





    figure out what should go here vs in proposed work section






    This section will provide more details about the technical work, if of interest. You can find the source code for this project at \href{https://github.com/jhleakakos/msds-data-mining-project}.

    src/orchestrator.py is the primary Python file, controlling the general flow of the pipeline. We do not need a full orchestration solution for this project, so we will recreate the idea, as needed, in this file, not worrying too much about automation but instead using this utility to help iterative work. One area of growth for larger or more complex systems would be to build a more complex orchestration file and infrastructure or incorporate orchestration software instead.

    The first milestone is getting the subset of IPEDS data that we want to work with loaded into the database. Since many real-world pipelines will start with data in a database or loading raw data into a database, we will recreate that here. For now, we run the database scripts manually.

    \begin{itemize}
        \item Download IPEDS flat files from the IPEDS website using src/pull_data.py. This script has code to pull all the files in the three domains of IPEDS data that we explore in this project. I originally downloaded and looked through all these files and their associated dictionaries, then reducing down to the files and features we want to explore. We comment out files that we do not plan to use in this iteration of the project. This script is easy to modify to pull in other IPEDS data flat files.
        \item Set up PostgreSQL to run in Docker. The root of the repository includes a Dockerfile that primarily copies the IPEDS files into the container along with a Docker Compose file that configures the remainder of the container and data volume.
        \item There are structured query language (SQL) scripts to create the datalake database, create the data lake tables, and copy in full IPEDS files as a stepping stone to pulling out the features in each file that we want to work keep.
        \item We load the data lake by using the \textit{copy} command. This takes in a comma-separated values (CSV) file and loads it into the specified table. This is purposely different than a process like reading the data into Python structures, opening a database connection, and using Python to write to the database. The idea here is that this method will let us load arbitrarily large datasets through a more efficient database utility.
        \item For now, we run the SQL scripts manually, though we could look to automate these later, likely through storing the data lake loading functionality as a stored procedure or setting up triggers to copy data around.
    \end{itemize}

    At this stage, we are ready to pivot to EDA and take a closer look at the subset of IPEDS data that we hope to eventually synthesize and feed into modeling.

\section{IPEDS Fields}
    The summaries for files and fields here come from each file's associated data dictionary. Note that the biggest difference between the list here and the larger summary in data/data_notes.md in the source repository is that we drop gender and race/ethnicity disaggregates here.

    Domain: Institutional Characteristics

    HDyyyy: directory information for institutions

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item INSTNM: institution name
        \item STABBR: state abbreviation
        \item OBEREG: Bureau of Economic Analysis regions, grouped by homogeneity of states based on economic, demographic, social, and cultural characteristics
        \item HLOFFER: highest level of credential offered
        \item DEGGRANT: does the institution offer degrees (as compared to non-degree credentials)
        \item HBCU: is the institution an HBCU
        \item TRIBAL: is this a tribal institution
        \item LOCALE: geographic status of school
        \item CLOSEDAT: date institution closed
        \item CYACTIVE: is institution active in current year
        \item INSTCAT: category of institution regarding credentials offered
        \item C21BASIC: alternative to Carnegie Classification and last updated in 2021
        \item INSTSIZE: enrollment size in 2019
        \item CBSA: core based statistical area
    \end{itemize}

    ICyyyy: program and award level offerings, control, and affiliation of institutions

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item CNTLAFFI: institution control or affiliation (public, private non-profit, etc)
        \item OPENADMP: open admission policy
    \end{itemize}

    ICyyyy_PY: student charges by program

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item PRGMOFR: number of programs offered at institution
        \item CIPCODE1: CIP code of the largest program
        \item CIPCODE2: CIP code of the second largest program
        \item CIPCODE3: CIP code of the third largest program
        \item CIPCODE4: CIP code of the fourth largest program
        \item CIPCODE5: CIP code of the fifth largest program
        \item CIPCODE6: CIP code of the sixth largest program
    \end{itemize}

    Domain: Enrollments

    EFFYyyyy: unduplicated 12-month enrollment headcount

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item EFFYALEV: level, full- and part-time status, degree- or nondegree-seeking status, and year of study of student enrolled for credit
        \item EFFYLEV: level of study
        \item EFYTOTLT: total students enrolled for credit
    \end{itemize}

    EFFYyyyy_DIST: unduplicated 12-month enrollment headcount by distance education status and level of student

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item EFFYDLEV: level and degree- or nondegree-seeking status of students enrolled for credit
        \item EFYDETOT: all students enrolled for credit (sum of next three columns)
        \item EFYDEEXC: students enrolled only in distance education courses
        \item EFYDESOM: students enrolled in at least one distance education course but not enrolled exclusively in distance education courses
        \item EFYDENON: students not enrolled in any distance education courses
    \end{itemize}

    EFFYyyyy_HS: unduplicated 12-month enrollment headcount of dual credit students; subset of the nondegree and non-certificate-seeking students from EFFY2023

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item EFYTOTLT: total dual credit students
    \end{itemize}

    EFIAyyyy: instructional activity measured in total credit and/or contact hours delivered by institutions in 12-month period, used to derive 12-month full time equivalent (FTE) enrollments

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item EFTEUG: estimated FTE undergraduate enrollment
        \item EFTEGD: estimated FTE graduate enrollment
        \item FTEUG: reported FTE undergraduate enrollment
        \item FTEGD: reported FTE graduate enrollment
        \item FTEDPP: doctor's-professional practice FTE
    \end{itemize}

    Domain: Completions

    Cyyyy_A: number of awards by type of program, level of award (degree or certificate), first or second major, by race/ethnicity, and by gender

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item CIPCODE: CIP code
        \item MAJORNUM: first or second major
        \item AWLEVEL: award level code
        \item CTOTALT: awards/degrees conferred to all recipients
    \end{itemize}

    Cyyyy_B: number of students who completed any degree or certificate by race/ethnicity and gender

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item CSTOTLT: number of students receiving awards/degrees
    \end{itemize}

    Cyyyy_C: number of students receiving a degree or certificate by level of award and race/ethnicity, gender, and age

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item AWLEVELC: award level
        \item CSTOTLT: number of students receiving awards/degrees
    \end{itemize}

    CyyyyDEP: number of programs offered by type of program, award level, and distance education status

    \begin{itemize}
        \item UNITID: unique institutional identifier
        \item CIPCODE: six-digit CIP code
        \item PTOTAL: number of programs offered across all award levels
        \item PTOTALDE: number of programs that can be completed entirely online across all award levels
        \item PASSOC: number of programs offered at associate's degree level
        \item PBACHL: number of programs offered at bachelor's degree level
        \item PMASTR: number of programs offered at master's degree level
        \item PDOCRS: number of programs offered at doctor's research/scholarship degree level
        \item PDOCPP: number of programs offered at doctor's professional practice degree level
        \item PDOCOT: number of programs offered at doctor's other degree level
        \item PCERT1A: number of programs offered at certificate level of less than 300 clock hours, 9 semester/trimester credit hours, or 13 quarter credit hours
        \item PCERT1B: number of programs offered at certificate level of 300-800 clock hours, 9-29 semester/trimester credit hours, or 13-44 quarter credit hours
        \item PCERT2: number of programs offered at certificate level of one year but less than two years
        \item PCERT4: number of programs offered at certificate level of two years but less than four years
        \item PPBACC: number of programs offered at postbaccalaureate certificate level
        \item PPMAST: number of programs offered at post-master's certificate level
    \end{itemize}

    \subsection{IPEDS Fields for Modeling}
        These are the fields we pull from the files above to create the modeling dataset.

        \begin{itemize}
            \item year: we snag this when reading in files and add it as needed throughout the warehouse
            \item HDyyyy.UNITID: unique institutional identifier
            \item HDyyyy.INSTNM: institution name
            \item HDyyyy.STABBR: state abbreviation
            \item HDyyyy.OBEREG: Bureau of Economic Analysis regions, grouped by homogeneity of states based on economic, demographic, social, and cultural characteristics
            \item HDyyyy.HLOFFER: highest level of credential offered
            \item HDyyyy.DEGGRANT: does the institution offer degrees (as compared to non-degree credentials)
            \item HDyyyy.HBCU: is the institution an HBCU
            \item HDyyyy.TRIBAL: is this a tribal institution
            \item HDyyyy.LOCALE: geographic status of school
            \item HDyyyy.CLOSEDAT: date institution closed
            \item HDyyyy.INSTCAT: category of institution regarding credentials offered
            \item HDyyyy.CBSA: core based statistical area
            \item ICyyyy.CNTLAFFI: institution control or affiliation (public, private non-profit, etc)
            \item EFFYyyyy.EFYTOTLT: total students enrolled for credit
            \item Cyyyy_B.CSTOTLT: number of students receiving awards/degrees
        \end{itemize}

\end{document}
\endinput
